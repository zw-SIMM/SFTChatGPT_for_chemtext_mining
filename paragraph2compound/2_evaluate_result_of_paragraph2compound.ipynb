{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从finetune的预测结果中读取文件评估、并保留评估结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph2compound_test_df = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "\n",
    "# Lists to store metrics for each row\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "jaccard_indices = []\n",
    "\n",
    "VERBOSE = False # True\n",
    "\n",
    "# Iterate through each row and compute metrics\n",
    "\n",
    "\n",
    "for index, row in paragraph2compound_test_df.iterrows():\n",
    "    paragraph_text = row['paragraphText']\n",
    "    ground_truth_entities = row['ground_truth']\n",
    "    predicted_entities = row['prediction']\n",
    "    \n",
    "    if VERBOSE:        \n",
    "        print(\"paragraph_text: \\n\", paragraph_text)\n",
    "        print(\"ground_truth_entities: \\n\", ground_truth_entities)\n",
    "        print(\"predicted_entities: \\n\", predicted_entities)\n",
    "\n",
    "    # Calculate TP, FP, FN\n",
    "    TP = len(set(ground_truth_entities) & set(predicted_entities))\n",
    "    FP = len(predicted_entities) - TP\n",
    "    FN = len(ground_truth_entities) - TP\n",
    "\n",
    "    if TP + FP == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / (TP + FP)\n",
    "        \n",
    "    if TP + FN == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = TP / (TP + FN)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f1score = 0.0\n",
    "    else:\n",
    "        f1score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    jaccard = TP / (TP + FP + FN)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1score)\n",
    "    jaccard_indices.append(jaccard)\n",
    "    \n",
    "    if VERBOSE:        \n",
    "        break\n",
    "\n",
    "# Calculate average metrics\n",
    "print(\"avg_Precision: \", np.mean(precisions))\n",
    "print(\"avg_Recall: \", np.mean(recalls))\n",
    "print(\"avg_F1_score: \", np.mean(f1_scores))\n",
    "print(\"avg_Jaccard_index: \", np.mean(jaccard_indices))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
